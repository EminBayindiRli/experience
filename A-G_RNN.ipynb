{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: ayni bu isi yapan bir RNN modeli deneyebilirz lutfen butun kod seklinde bana. bu modeli olusturur musun\n",
        "\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim import downloader as api\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "!pip install openpyxl\n",
        "\n",
        "\n",
        "df = pd.read_excel('Euraxess_Satcom.xlsx')\n",
        "\n",
        "columns = ['Title', 'OfferDescription', 'Requirements', 'Responsibilities', 'AdditionalInformation']\n",
        "existing_columns = [col for col in columns if col in df.columns]\n",
        "\n",
        "\n",
        "# Combine relevant columns into a single text column, handling missing values\n",
        "df['combined_text'] = df[existing_columns].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "\n",
        "# Handle empty combined texts\n",
        "df['combined_text'] = df['combined_text'].apply(lambda x: x if x else 'NA')\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized_text'] = df['combined_text'].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)]))\n",
        "\n",
        "# Keyword-based labeling\n",
        "keywords = [\n",
        "    'aerospace', 'geolocation', 'satellite', 'navigation', 'GPS', 'GNSS', 'GIS',\n",
        "    'remote sensing', 'UAV', 'drone', 'positioning', 'earth observation', 'orbit',\n",
        "    'launch vehicle', 'spacecraft', 'cosmonaut', 'astronaut', 'cartography',\n",
        "    'geospatial', 'surveying', 'remote sensing imagery', 'geodetic', 'positioning system',  'space', 'geodesy', 'mapping', 'photogrammetry', 'lidar', 'radar', 'earth science',\n",
        "    'geophysical', 'geospatial analysis', 'location based services', 'lbs', 'navigation system',\n",
        "    'satellite imagery', 'aerial imagery', 'geospatial data', 'geodata', 'geomatics'\n",
        "]\n",
        "df['label'] = df['lemmatized_text'].apply(lambda text: 1 if any(keyword in text.lower() for keyword in keywords) else 0)\n",
        "\n",
        "# Word2Vec embeddings\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def sentence_to_vec(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    vectors = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
        "\n",
        "df['word2vec'] = df['lemmatized_text'].apply(sentence_to_vec)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['word2vec'].tolist(), df['label'], test_size=0.2, random_state=42)\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word2vec_model.key_to_index), 300, weights=[word2vec_model.vectors], trainable=False))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Predict for the whole dataset\n",
        "df['prediction'] = (model.predict(df['word2vec'].tolist()) > 0.5).astype(int)\n",
        "print(df['prediction'].value_counts())\n",
        "\n",
        "# Inspect misclassified samples\n",
        "false_negatives = df[(df['label'] == 1) & (df['prediction'] == 0)]\n",
        "print(false_negatives[['lemmatized_text', 'label', 'prediction']])\n"
      ],
      "metadata": {
        "id": "vP382HQJE3gD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}