{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem import WordNetLemmatizer\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-bMr5VeYmmM",
        "outputId": "09c2f3d0-faf4-4ca6-b0bb-edc8652aa89f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_excel('SpaceIndividuals_GNSS.xlsx')\n",
        "\n",
        "columns = ['Title', 'OfferDescription', 'Requirements', 'Responsibilities', 'AdditionalInformation']\n",
        "existing_columns = [col for col in columns if col in df.columns]\n",
        "\n",
        "\n",
        "input_texts = df[existing_columns].apply(lambda x: '. '.join(x.dropna().astype(str)), axis=1).tolist()\n",
        "input_texts = [text if text else 'NA' for text in input_texts]"
      ],
      "metadata": {
        "id": "mcvPDjtYYplf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Apply lemmatization to input texts\n",
        "input_texts = [lemmatize_text(text) for text in input_texts]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
        "X = vectorizer.fit_transform(input_texts)"
      ],
      "metadata": {
        "id": "5bvSuVDrYri2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_label(text):\n",
        "    keywords = [\n",
        "    'aerospace', 'geolocation', 'satellite', 'navigation', 'GPS', 'GNSS', 'GIS',\n",
        "    'remote sensing', 'UAV', 'drone', 'positioning', 'earth observation', 'orbit',\n",
        "    'launch vehicle', 'spacecraft', 'cosmonaut', 'astronaut', 'cartography',\n",
        "    'geospatial', 'surveying', 'remote sensing imagery', 'geodetic', 'positioning system']\n",
        "    return 1 if any(keyword in text.lower() for keyword in keywords) else 0\n",
        "\n",
        "\n",
        "y = [determine_label(text) for text in input_texts]"
      ],
      "metadata": {
        "id": "XZqeb5boYt6O"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim import downloader as api\n",
        "\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def sentence_to_vec(sentence):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    vectors = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index] # Use key_to_index instead of .vocab\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(300)  # Vektör boyutuna göre ayarlayın"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rgg6awqYvuF",
        "outputId": "f66be2db-6723-4c3a-e1f3-f629d9456b47"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Cümleleri vektörlere dönüştürün\n",
        "sentence_vectors = [sentence_to_vec(sentence) for sentence in input_texts]\n",
        "\n",
        "# Scikit-learn'e uygun bir formata dönüştürün\n",
        "X = np.array(sentence_vectors)\n",
        "\n",
        "# Assuming 'input_texts' is your original list of text documents\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_texts, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAFqw_I1Ykz4",
        "outputId": "cca36de4-7666-47fe-c45d-cb08deb1c00a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 1.0\n",
            "F1-score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: cikan sonuclari dataya yeni bir column olarak kaydet `\n",
        "\n",
        "# Tahminleri DataFrame'e ekleyin\n",
        "df['cikartik'] = pipeline.predict(input_texts)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lT3UhadpZBie"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: df['cikartik'] daki 0  ve 1 leri goster\n",
        "\n",
        "print(df['cikartik'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja3etz-OiLoy",
        "outputId": "a46318ae-a8e3-406d-cf21-2b461de2e607"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cikartik\n",
            "1    54\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYIu-AmWiR_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
